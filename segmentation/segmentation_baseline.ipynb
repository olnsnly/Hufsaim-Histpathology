{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfde0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "import glob\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import datetime\n",
    "import openslide\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import RandomSampler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from monai.losses import DiceCELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fd3e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASAP for python\n",
    "sys.path.append(\"/home/bmegpu01/wjin/ASAP/opt/ASAP/bin\")\n",
    "import multiresolutionimageinterface as mir\n",
    "\n",
    "# use mir\n",
    "annotation_list = mir.AnnotationList()\n",
    "reader = mir.MultiResolutionImageReader()\n",
    "\n",
    "# set level\n",
    "patch_level = 7\n",
    "\n",
    "# load bbox position\n",
    "json_path0 = \"./Data/LN_bbox_position.json\"\n",
    "with open(json_path0, 'r') as file:\n",
    "    bbox_position = json.load(file)\n",
    "    \n",
    "json_path1 = \"./Data/LN_label_bbox_position.json\"\n",
    "with open(json_path1, 'r') as file:\n",
    "    label_bbox_position = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf747aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transform():\n",
    "    '''\n",
    "    Train transform\n",
    "    '''\n",
    "    return T.Compose([\n",
    "        T.RandomApply([T.RandomRotation(180)],0.3),\n",
    "        T.RandomApply([T.RandomHorizontalFlip()],0.2),\n",
    "        T.ToTensor(),\n",
    "    ])\n",
    "\n",
    "def valid_transform():\n",
    "    '''\n",
    "    Valid transform\n",
    "    '''\n",
    "    return T.Compose([T.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb50df3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_patch(img:openslide.OpenSlide, bbox_dict:dict, patch_level:int ,patch_size:tuple, type_str = 'RGB') -> Image.Image:\n",
    "    '''\n",
    "    The fuction that extracts random size patches.\n",
    "    \n",
    "    '''\n",
    "    n = 2**bbox_dict['lv']\n",
    "    c0 = 0\n",
    "    while(c0 == 0):\n",
    "        \n",
    "        x = random.randint(bbox_dict['x1']*n, bbox_dict['x2']*n-patch_size[0])\n",
    "        y = random.randint(bbox_dict['y1']*n, bbox_dict['y2']*n-patch_size[1])\n",
    "        \n",
    "        start_point = (y,x)\n",
    "        \n",
    "        img_patch = img.read_region(start_point, patch_level, patch_size).convert(type_str)\n",
    "        c0 = np.count_nonzero(np.array(img_patch))\n",
    "        \n",
    "\n",
    "    return img_patch, start_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45946dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpplv_list = [0.5, 1.0, 2.0, 4.0, 8.0, 16.0, 32.0, 64.0, 128.0, 256.0, 512.0]\n",
    "\n",
    "class selfDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, img_list, label_list, patch_level, bbox_position, label_bbox_position, transforms = None):\n",
    "        \n",
    "        super().__init__()\n",
    "    \n",
    "        self.img_list = img_list\n",
    "        self.label_list = label_list\n",
    "        self.patch_level = patch_level\n",
    "        \n",
    "        self.transforms = transforms\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index)->(torch.tensor, torch.tensor):\n",
    "        \n",
    "        img_path = self.img_list[index]\n",
    "        label_path = self.label_list[index]\n",
    "        patch_level = self.patch_level\n",
    "        \n",
    "        img = openslide.OpenSlide(img_path)\n",
    "        label = openslide.OpenSlide(label_path)\n",
    "        \n",
    "        # read_region parameters\n",
    "        img_id = img_path.split('.mrxs')[0].split('/')[-1]\n",
    "        bbox_dict = bbox_position[img_id]\n",
    "        label_bbox_dict = label_bbox_position[img_id+'_label']\n",
    "        patch_size = (256, 256)\n",
    "        \n",
    "        \n",
    "        if index & 1:\n",
    "            img_patch, start_point = random_patch(img, bbox_dict, patch_level, patch_size)\n",
    "            label_patch = label.read_region(start_point, patch_level, patch_size).convert('L')\n",
    "            \n",
    "        else:\n",
    "            label_patch, start_point = random_patch(label, label_bbox_dict ,patch_level, patch_size, type_str ='L')\n",
    "            img_patch = img.read_region(start_point, patch_level, patch_size).convert('RGB')\n",
    "        \n",
    "        \n",
    "        ### Applying transforms on image\n",
    "        if self.transforms:\n",
    "            seed = np.random.randint(2147483647)\n",
    "            \n",
    "            random.seed(seed)\n",
    "            torch.manual_seed(seed)\n",
    "            img_patch = self.transforms(img_patch)\n",
    "            \n",
    "            random.seed(seed)\n",
    "            torch.manual_seed(seed)\n",
    "            label_patch = self.transforms(label_patch)\n",
    "        \n",
    "        label_patch[label_patch>0]=1\n",
    "        \n",
    "#         label_patch= np.array(label_patch)\n",
    "#         label_patch = label_patch[np.newaxis, ...]\n",
    "#         label_patch = torch.from_numpy(label_patch)\n",
    "        \n",
    "        return img_patch, label_patch\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352d725c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test data\n",
    "path0 = './Data/LN_metastasis'\n",
    "path1 = './Data/LN_label'\n",
    "\n",
    "image_list = np.sort(glob.glob(path0+'/*.mrxs'))\n",
    "label_list = np.sort(glob.glob(path1+'/*.mrxs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a867d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = selfDataset(image_list, label_list, patch_level, bbox_position, label_bbox_position, train_transform())\n",
    "train_random_sampler = RandomSampler(train_dataset)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset = train_dataset,\n",
    "    batch_size = 10,\n",
    "    sampler = train_random_sampler,\n",
    "    num_workers = 0,\n",
    ")\n",
    "\n",
    "valid_dataset = selfDataset(image_list, label_list, patch_level, bbox_position, label_bbox_position, valid_transform())\n",
    "valid_random_sampler = RandomSampler(valid_dataset)\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    dataset = valid_dataset,\n",
    "    batch_size = 4,\n",
    "    sampler = valid_random_sampler,\n",
    "    num_workers = 0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f491d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_patch, label_patch in train_loader:\n",
    "    fig, img = plt.subplots(figsize = (8, 8))\n",
    "    img.set_xticks([])\n",
    "    img.set_yticks([])\n",
    "    img.imshow(make_grid(img_patch,5).permute(1, 2, 0))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdd60b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "    \n",
    "    \n",
    "class DownBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DownBlock, self).__init__()\n",
    "        self.double_conv = DoubleConv(in_channels, out_channels)\n",
    "        self.down_sample = nn.MaxPool2d(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_out = self.double_conv(x)\n",
    "        down_out = self.down_sample(skip_out)\n",
    "        return (down_out, skip_out)\n",
    "\n",
    "    \n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, up_sample_mode):\n",
    "        super(UpBlock, self).__init__()\n",
    "        if up_sample_mode == 'conv_transpose':\n",
    "            self.up_sample = nn.ConvTranspose2d(in_channels-out_channels, in_channels-out_channels, kernel_size=2, stride=2)        \n",
    "        elif up_sample_mode == 'bilinear':\n",
    "            self.up_sample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported `up_sample_mode` (can take one of `conv_transpose` or `bilinear`)\")\n",
    "        self.double_conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, down_input, skip_input):\n",
    "        x = self.up_sample(down_input)\n",
    "        x = torch.cat([x, skip_input], dim=1)\n",
    "        return self.double_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687ad31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, out_classes=1, up_sample_mode='conv_transpose'):\n",
    "        super(UNet, self).__init__()\n",
    "        self.up_sample_mode = up_sample_mode\n",
    "        # Downsampling Path\n",
    "        self.down_conv1 = DownBlock(3, 64)\n",
    "        self.down_conv2 = DownBlock(64, 128)\n",
    "        self.down_conv3 = DownBlock(128, 256)\n",
    "        self.down_conv4 = DownBlock(256, 512)\n",
    "        # Bottleneck\n",
    "        self.double_conv = DoubleConv(512, 1024)\n",
    "        # Upsampling Path\n",
    "        self.up_conv4 = UpBlock(512 + 1024, 512, self.up_sample_mode)\n",
    "        self.up_conv3 = UpBlock(256 + 512, 256, self.up_sample_mode)\n",
    "        self.up_conv2 = UpBlock(128 + 256, 128, self.up_sample_mode)\n",
    "        self.up_conv1 = UpBlock(128 + 64, 64, self.up_sample_mode)\n",
    "        # Final Convolution\n",
    "        self.conv_last = nn.Conv2d(64, out_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, skip1_out = self.down_conv1(x)\n",
    "        x, skip2_out = self.down_conv2(x)\n",
    "        x, skip3_out = self.down_conv3(x)\n",
    "        x, skip4_out = self.down_conv4(x)\n",
    "        x = self.double_conv(x)\n",
    "        x = self.up_conv4(x, skip4_out)\n",
    "        x = self.up_conv3(x, skip3_out)\n",
    "        x = self.up_conv2(x, skip2_out)\n",
    "        x = self.up_conv1(x, skip1_out)\n",
    "        x = self.conv_last(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beef7475",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = UNet()\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-5)\n",
    "loss_function = DiceCELoss(to_onehot_y=True, softmax=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7911bb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, global_step, train_loader, dice_val_best, global_step_best):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    epoch_iterator = tqdm(\n",
    "        train_loader, desc=\"Training (X / X Steps) (loss=X.X)\", dynamic_ncols=True\n",
    "    )\n",
    "    \n",
    "    for step, (images, labels) in enumerate(train_loader):\n",
    "        step += 1\n",
    "        \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        \n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        epoch_iterator.set_description(\n",
    "            \"Training (%d / %d Steps) (loss=%2.5f)\" % (global_step, max_iterations, epoch_loss/step)\n",
    "        )\n",
    "        \n",
    "    if global_step % eval_num == 0 or global_step == max_iterations:\n",
    "#         epoch_iterator_val = tqdm(\n",
    "#             valid_loader, desc=\"Validate (X / X Steps) (loss=X.X)\", dynamic_ncols=True\n",
    "#         )\n",
    "        dice_val, val_labels, val_outputs = validation(model)\n",
    "        epoch_loss /= step\n",
    "        epoch_loss_values.append(epoch_loss)\n",
    "        metric_values.append(dice_val)\n",
    "        plt.subplot(121)\n",
    "        plt.plot(epoch_loss_values)\n",
    "        plt.title(f'step #: {global_step}')\n",
    "        plt.subplot(122)\n",
    "        plt.plot(metric_values)\n",
    "        plt.show()\n",
    "\n",
    "        if dice_val > dice_val_best:\n",
    "            dice_val_best = dice_val\n",
    "            global_step_best = global_step\n",
    "            torch.save(\n",
    "                model.state_dict(), os.path.join( f\"test2.pth\")\n",
    "            )\n",
    "            print(\n",
    "                \"Model Was Saved ! Current Best Avg. IoU: {} Current Avg. IoU: {}\".format(\n",
    "                    dice_val_best, dice_val\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            print(\n",
    "                \"Model Was Not Saved ! Current Best Avg. IoU: {} Current Avg. IoU: {}\".format(\n",
    "                    dice_val_best, dice_val\n",
    "                )\n",
    "            )\n",
    "\n",
    "        show_result(val_labels, val_outputs, global_step)\n",
    "    \n",
    "    global_step += 1\n",
    "\n",
    "    return global_step, dice_val_best, global_step_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fb3238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model):\n",
    "    model.eval()\n",
    "    dice_vals = list()\n",
    "    with torch.no_grad():\n",
    "        for step, (images, labels) in enumerate(valid_loader):\n",
    "            step += 1\n",
    "            \n",
    "            val_images = images.to(device)\n",
    "            val_labels = labels.to(device)\n",
    "            val_outputs = model(val_images)\n",
    "            val_loss = loss_function(val_outputs, val_labels)\n",
    "           \n",
    "            dice_vals.append(val_loss.cpu())\n",
    "#             epoch_iterator_val.set_description(\n",
    "#                 \"Validate (%d / %d Steps) (dice=%2.5f)\" % (global_step, max_iterations, val_loss)\n",
    "#             )\n",
    "                  \n",
    "    mean_dice_val = np.mean(dice_vals)\n",
    "    return mean_dice_val, val_labels, val_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c277ab6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_result(val_labels, val_outputs, global_step):\n",
    "    plt.figure(2,figsize=(24,16))\n",
    "    plt.subplot(131)\n",
    "    plt.imshow(val_outputs[0,0,:,:].cpu())\n",
    "    plt.title('prediction')\n",
    "    plt.subplot(132)\n",
    "    plt.imshow((val_outputs[0,0,:,:]>.5).cpu())\n",
    "    plt.title('prediction > 0.5')\n",
    "    plt.subplot(133)\n",
    "    plt.imshow(val_labels[0,0,:,:].cpu())\n",
    "    plt.title('label')\n",
    "    tmpa = datetime.datetime.now().date()\n",
    "    plt.savefig(f'Result/test/{tmpa}_step_{global_step}.png')\n",
    "    plt.close(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b556d192",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iterations =  25000\n",
    "eval_num = 500\n",
    "\n",
    "global_step = 0\n",
    "dice_val_best = 0.0\n",
    "global_step_best = 0\n",
    "epoch_loss_values = []\n",
    "metric_values = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1996073b",
   "metadata": {},
   "outputs": [],
   "source": [
    "while global_step < max_iterations:\n",
    "    global_step, dice_val_best, global_step_best = train(\n",
    "        model, global_step, train_loader, dice_val_best, global_step_best\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9 (default, Apr 13 2022, 08:48:06) \n[Clang 13.1.6 (clang-1316.0.21.2.5)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
